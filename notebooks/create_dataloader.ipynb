{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc715cf",
   "metadata": {},
   "source": [
    "# Cleaning Data and Creating DataLoader Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad075ef",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34672e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from importlib.metadata import version\n",
    "from logging import Logger\n",
    "from typing import List, Optional\n",
    "import logging\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from pandas.errors import ParserError\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db165c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6886c558",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = [\"pandas\", \"importlib-metadata\", \"pyarrow\"]\n",
    "for package in packages:\n",
    "    try:\n",
    "        logger.info(f\"{package} version: {version(package)}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not get version for package {package}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6040322d",
   "metadata": {},
   "source": [
    "## Load Dataframe from csv file in local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c642d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"../Data\")\n",
    "RAW_DATA_DIR_NAME = \"Downloaded-Data\"\n",
    "# DATA_RAW_FILE_NAME = \"credit-card-fraud.csv\"\n",
    "\n",
    "DATA_RAW_FILE_NAME = \"ENTER A DATA FILE NAME (e.g., data-RAW.csv)\"\n",
    "DATA_CLEAN_FILE_NAME = \"ENTER A DATA FILE NAME (e.g., data-CLEAN.csv)\"\n",
    "\n",
    "RAW_DATA_PATH = DATA_ROOT / RAW_DATA_DIR_NAME / DATA_RAW_FILE_NAME\n",
    "DATA_PATH = DATA_ROOT / RAW_DATA_DIR_NAME / DATA_CLEAN_FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(RAW_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50211765",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7631a6e8",
   "metadata": {},
   "source": [
    "### Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3845851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_these = [\n",
    "    \"ADD\",\n",
    "    \"COLUMNS\",\n",
    "    \"TO\",\n",
    "    \"DROP\",\n",
    "    \"HERE\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17112664",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=drop_these, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3604cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee99efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a12bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3899cfb",
   "metadata": {},
   "source": [
    "### Create Dictionaries for Encoding/Mapping Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f39d9",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b68946",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df[\"category\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833836be",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d827b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cba6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_dict = {category: float(idx) for idx, category in enumerate(categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d334bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e029de17",
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = sorted(df[\"gender\"].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dict = {gender: float(idx) for idx, gender in enumerate(genders)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d7a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = sorted(df[\"state\"].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac4421",
   "metadata": {},
   "outputs": [],
   "source": [
    "states.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f74d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_dict = {state: float(idx) for idx, state in enumerate(states)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba3204",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e33f975",
   "metadata": {},
   "source": [
    "### Apply Encoding/Mapping to the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"category\"] = df[\"category\"].map(categories_dict)\n",
    "df[\"gender\"] = df[\"gender\"].map(gender_dict)\n",
    "df[\"state\"] = df[\"state\"].map(states_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d71986",
   "metadata": {},
   "source": [
    "### Convert all columns into Specific datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c7589",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068feac3",
   "metadata": {},
   "source": [
    "### Print details of the Final Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe78d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4942a60b",
   "metadata": {},
   "source": [
    "### End of Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15d158a",
   "metadata": {},
   "source": [
    "# SAVING CLEANED DATA TO FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ad1eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DATA_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc593cc",
   "metadata": {},
   "source": [
    "# Read and Test datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8bcb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dq = pd.read_csv(\n",
    "    DATA_PATH, dtype=\"float32\"\n",
    ")  # Does not convert to float32 by default, dtype has to be explicitly provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db954ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf62b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dq.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c997b2c7",
   "metadata": {},
   "source": [
    "# Creating DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41323816",
   "metadata": {},
   "source": [
    "### Clean Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4e6fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(\n",
    "    df: pd.DataFrame, logger: Logger, extra_dropped_columns: Optional[List[str]] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Cleans the input DataFrame.\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame to be cleaned.\n",
    "        logger (Logger): Logger object for logging information.\n",
    "        extra_dropped_columns (List[str], optional): Columns to drop from the features in original dataset.\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    show_dataframe_info = True  # Set to True to log DataFrame info\n",
    "\n",
    "    # Log the initial state of the DataFrame\n",
    "    logger.info(f\"Initial DataFrame shape: {df.shape}\")\n",
    "\n",
    "    if show_dataframe_info:\n",
    "        buffer = io.StringIO()  # Create a buffer to capture the info output\n",
    "        df.info(buf=buffer)  # Store the output into the buffer\n",
    "        logger.info(f\"Initial DataFrame info:\\n \" + buffer.getvalue())\n",
    "\n",
    "    # Drop any unused columns\n",
    "    try:\n",
    "        df.drop(columns=extra_dropped_columns, inplace=True)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Problem dropping columns:\\n{e}\")\n",
    "\n",
    "    # Create dictionaries for mapping/encoding\n",
    "\n",
    "    # ================================\n",
    "    # EXAMPLE PROCESS\n",
    "    # ================================\n",
    "\n",
    "    categories = sorted(df[\"category\"].unique().tolist())\n",
    "    categories_dict = {category: idx for idx, category in enumerate(categories)}\n",
    "\n",
    "    genders = sorted(df[\"gender\"].unique().tolist())\n",
    "    gender_dict = {gender: idx for idx, gender in enumerate(genders)}\n",
    "\n",
    "    states = sorted(df[\"state\"].unique().tolist())\n",
    "    states_dict = {state: idx for idx, state in enumerate(states)}\n",
    "\n",
    "    logger.info(\"Encoding categorical variables...\")\n",
    "    try:\n",
    "        df[\"category\"] = df[\"category\"].map(categories_dict)\n",
    "        df[\"gender\"] = df[\"gender\"].map(gender_dict)\n",
    "        df[\"state\"] = df[\"state\"].map(states_dict)\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Problem encoding columns, {e}\")\n",
    "\n",
    "    # ================================\n",
    "    # END OF MAPPING/ENCODING EXAMPLE\n",
    "    # ================================\n",
    "\n",
    "    # Handle missing values (if any)\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        logger.info(\"Handling missing values...\")\n",
    "        df = df.dropna()  # Example: Drop rows with missing values\n",
    "        logger.info(f\"DataFrame shape after dropping missing values: {df.shape}\")\n",
    "\n",
    "    # Convert to 'float32' to reduce memory usage\n",
    "    logger.info(\"Converting Entire Data Frame to 'float32'...\")\n",
    "    df = df.astype(\"float32\")\n",
    "\n",
    "    if show_dataframe_info:\n",
    "        # Reinitialize the buffer to clear any previous content in order to log the final dataframe info\n",
    "        buffer = io.StringIO()\n",
    "        df.info(buf=buffer)\n",
    "        logger.info(f\"Final DataFrame info:\\n \" + buffer.getvalue())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c86fb",
   "metadata": {},
   "source": [
    "### Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cd3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Dataset class For the Custom Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file: str = \"../Data/DataSplits/test.csv\", label_column: str = \"Label\"):\n",
    "        \"\"\"Initializer for the Dataset class.\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file containing the dataset.\n",
    "            label_column (str): The name of the column indicating the label.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.data = pd.read_csv(csv_file)  # Assign a pandas data frame\n",
    "        except FileNotFoundError:  # Raise an error if the file is not found\n",
    "            raise FileNotFoundError(f\"File not found: {csv_file}\")\n",
    "\n",
    "        # Define feature and label columns\n",
    "        self.label_column = label_column\n",
    "        # Omit the label column to create the list of feature columns\n",
    "        self.feature_columns = self.data.columns.drop([self.label_column])\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Returns a tuple (features, label) for the given index.\n",
    "        Args:\n",
    "            index (int): Index of the data sample to retrieve.\n",
    "        Returns:\n",
    "            tuple: (features, label) where features is a tensor of input features and label is the corresponding label.\n",
    "        \"\"\"\n",
    "        # Use 'iloc' instead of 'loc' for efficiency\n",
    "        features = self.data.iloc[index][self.feature_columns].values\n",
    "        label = self.data.iloc[index][self.label_column]  # Extract the label for the given index\n",
    "        return (torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.long))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the amount of samples in the dataset.\"\"\"\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c267da",
   "metadata": {},
   "source": [
    "### Data Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df05a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(\n",
    "    logger: Logger,\n",
    "    dataset_url: str,\n",
    "    root_data_dir: str = \"../Data\",\n",
    "    data_file_path: str = \"Dataset.csv\",\n",
    "    data_splits_dir: str = \"DataSplits\",\n",
    "    scaler_dir=\"Scalers\",\n",
    "    target_column: str = \"Target\",\n",
    "    extra_dropped_columns: Optional[List[str]] = None,\n",
    "    batch_size: int = 64,\n",
    "    num_workers: int = 0,\n",
    "    pin_memory: bool = False,\n",
    "    drop_last: bool = True,\n",
    ") -> tuple[\n",
    "    Dataset, Dataset, Dataset, DataLoader, DataLoader, DataLoader, MinMaxScaler, MinMaxScaler\n",
    "]:\n",
    "    \"\"\"This function prepares the train, test, and validation datasets.\n",
    "    Args:\n",
    "        logger (Logger): The logger instance to log messages.\n",
    "        dataset_url (str): The URL to download the dataset from, if not found locally.\n",
    "        root_data_dir (str): The root of the Data Directory\n",
    "        data_file_path (str): The name of the original dataset (with .csv file extension).\n",
    "        data_splits_dir (str): Path to the train, test, and validation datasets.\n",
    "        scaler_dir (str): Path to the feature and label scalers.\n",
    "        target_column (str): The name of the target column to predict.\n",
    "        extra_dropped_columns (List[str], optional): Columns to drop from the features in original dataset.\n",
    "        batch_size (int): The dataloader's batch_size.\n",
    "        num_workers (int): The dataloader's number of workers.\n",
    "        pin_memory (bool): The dataloader's pin memory option.\n",
    "        drop_last (bool): The dataloader's drop_last option.\n",
    "\n",
    "    Returns:\n",
    "        train_dataset (Dataset): Dataset Class for the training dataset.\n",
    "        test_dataset (Dataset): Dataset Class for the test dataset.\n",
    "        validation_dataset (Dataset): Dataset Class for the validation dataset.\n",
    "        train_dataloader (DataLoader): The train dataloader.\n",
    "        test_dataloader (DataLoader): The test dataloader.\n",
    "        validation_dataloader (DataLoader): The validation dataloader.\n",
    "        feature_scaler (MinMaxScaler): The scaler used to scale the features of the model input.\n",
    "        label_scaler (MinMaxScaler): The scaler used to scale the labels of the model input.\n",
    "    \"\"\"\n",
    "    if (\n",
    "        not root_data_dir or not data_file_path or not data_splits_dir\n",
    "    ):  # Check for empty strings at the beginning\n",
    "        raise ValueError(\"File and directory paths cannot be empty strings.\")\n",
    "    DATA_ROOT = Path(root_data_dir)\n",
    "\n",
    "    DATA_CLEAN_PATH = DATA_ROOT / data_file_path  # Set the path to the complete dataset\n",
    "\n",
    "    if DATA_CLEAN_PATH.exists():\n",
    "        logger.info(f\"CSV file detected, reading from '{DATA_ROOT}'\")\n",
    "        df = pd.read_csv(\n",
    "            DATA_CLEAN_PATH, dtype=\"float32\"\n",
    "        )  # Convert data to float32 instead of, float64\n",
    "    else:\n",
    "        logger.info(f\"Downloading CSV file from '{dataset_url}'\\nand saving into '{DATA_ROOT}'\")\n",
    "        try:\n",
    "            os.makedirs(DATA_ROOT, exist_ok=True)  # Create the Data Root Directory\n",
    "            # Download and read the data into a pandas dataframe\n",
    "            df = pd.read_csv(dataset_url)  # Keep data as is, may not be able to expect float32 data\n",
    "\n",
    "            # Clean the data before saving\n",
    "            try:\n",
    "                df = clean_data(df, logger, extra_dropped_columns=extra_dropped_columns)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"An unexpected error occurred cleaning the dataset:\\n{e}\")\n",
    "\n",
    "            df.to_csv(DATA_CLEAN_PATH, index=False)  # Save the file, omitting saving the row index\n",
    "        except OSError as e:\n",
    "            raise RuntimeError(f\"OS error occurred: {e}\")\n",
    "        except ParserError:\n",
    "            raise RuntimeError(f\"Failed to parse CSV from '{dataset_url}'\")\n",
    "        except ValueError as e:\n",
    "            raise RuntimeError(f\"Data cleaning error:\\n{e}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"An unexpected error occurred when downloading or saving the \"\n",
    "                f\"dataset from '{dataset_url}' to '{DATA_CLEAN_PATH}':\\n{e}\"\n",
    "            )\n",
    "\n",
    "    # Define the paths for the data splits and scalers\n",
    "    DATA_SPLITS_DIR = DATA_ROOT / data_splits_dir\n",
    "    SCALER_DIR = DATA_ROOT / scaler_dir\n",
    "\n",
    "    TRAIN_DATA_PATH = DATA_SPLITS_DIR / \"train.csv\"\n",
    "    TEST_DATA_PATH = DATA_SPLITS_DIR / \"test.csv\"\n",
    "    VALIDATION_DATA_PATH = DATA_SPLITS_DIR / \"val.csv\"\n",
    "\n",
    "    FEATURE_SCALER_PATH = SCALER_DIR / \"feature-scaler.joblib\"\n",
    "    LABEL_SCALER_PATH = SCALER_DIR / \"label-scaler.joblib\"\n",
    "\n",
    "    # Dictate whether to use label scaler\n",
    "    USE_LABEL_SCALER = False  # TOGGLE IF NEEDED\n",
    "\n",
    "    # Define the columns to drop from the features\n",
    "    columns_to_drop = [target_column]\n",
    "\n",
    "    # Define the Data Splits\n",
    "    TRAIN_SPLIT_PERCENTAGE = 0.9\n",
    "    VALIDATION_SPLIT_PERCENTAGE = 0.5\n",
    "\n",
    "    if (\n",
    "        os.path.exists(TRAIN_DATA_PATH)\n",
    "        and os.path.exists(TEST_DATA_PATH)\n",
    "        and os.path.exists(VALIDATION_DATA_PATH)\n",
    "    ):\n",
    "        logger.info(\n",
    "            f\"Train, Test, and Validation CSV datasets detected in '{DATA_SPLITS_DIR}.' Skipping generation and loading scaler(s)\"\n",
    "        )\n",
    "        try:\n",
    "            feature_scaler = joblib.load(FEATURE_SCALER_PATH)\n",
    "            logger.info(f\"Feature scaler stored in: ({FEATURE_SCALER_PATH})\")\n",
    "            if USE_LABEL_SCALER:\n",
    "                joblib.dump(\n",
    "                    label_scaler, LABEL_SCALER_PATH\n",
    "                )  # Not used for this classification task\n",
    "                logger.info(f\"Label scaler stored in: ({LABEL_SCALER_PATH})\")\n",
    "            else:\n",
    "                label_scaler = None  # Omit the label scaler loading\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            raise RuntimeError(f\"Scaler file not found: {e}\")\n",
    "        except EOFError as e:\n",
    "            raise RuntimeError(f\"Scaler file appears to be empty or corrupted: {e}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An unexpected error occurred when loading scalers: {e}\")\n",
    "    else:\n",
    "        logger.info(\n",
    "            f\"Datasets not found in '{DATA_SPLITS_DIR}' or incomplete. Generating datasets...\"\n",
    "        )\n",
    "        os.makedirs(DATA_SPLITS_DIR, exist_ok=True)  # Create the Data Splits Parent Directory\n",
    "        os.makedirs(SCALER_DIR, exist_ok=True)  # Create the Scaler Parent Directory\n",
    "\n",
    "        # Create the scaler objects\n",
    "        feature_scaler = MinMaxScaler()\n",
    "        if USE_LABEL_SCALER:\n",
    "            label_scaler = MinMaxScaler()\n",
    "        else:\n",
    "            label_scaler = None  # Not used for this Classification task\n",
    "\n",
    "        try:\n",
    "            df_features = df.drop(columns=columns_to_drop, inplace=False)\n",
    "            df_labels = df[\n",
    "                [target_column]\n",
    "            ]  # Instead of returning a pandas Series using \"[]\", return a dataframe using the \"[[]]\" to get a shape with (-1,1)\n",
    "        except KeyError as e:\n",
    "            raise KeyError(\n",
    "                f\"One or more specified columns to drop do not exist in the DataFrame: {e}\"\n",
    "            )\n",
    "\n",
    "        # ================================\n",
    "        # ADD OVERSAMPLING AND OTHER DATA BALANCING TECHNIQUES HERE\n",
    "        # ================================\n",
    "\n",
    "        # Example of using OverSampling Technique to Balance out the Dataset for an Unbalanced Dataset\n",
    "        ros = RandomOverSampler(random_state=42)\n",
    "        df_features_resampled, df_labels_resampled = ros.fit_resample(df_features, df_labels)\n",
    "\n",
    "        # Split into smaller DataFrames for the Train, Test, and Validation splits\n",
    "        X_train, X_inter, Y_train, Y_inter = train_test_split(\n",
    "            df_features_resampled,\n",
    "            df_labels_resampled,\n",
    "            test_size=1 - TRAIN_SPLIT_PERCENTAGE,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        # ================================\n",
    "        # END  OF OVERSAMPLING AND OTHER DATA BALANCING TECHNIQUES ; OTHERWISE\n",
    "        # ================================\n",
    "\n",
    "        # Split into smaller DataFrames for the Train, Test, and Validation splits\n",
    "        X_train, X_inter, Y_train, Y_inter = train_test_split(\n",
    "            df_features,\n",
    "            df_labels,\n",
    "            test_size=1 - TRAIN_SPLIT_PERCENTAGE,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        X_validation, X_test, Y_validation, Y_test = train_test_split(\n",
    "            X_inter, Y_inter, test_size=1 - VALIDATION_SPLIT_PERCENTAGE, random_state=42\n",
    "        )\n",
    "\n",
    "        # Fit the scalers to the data\n",
    "        feature_scaler.fit(X_train)\n",
    "        # Only scale the labels if required\n",
    "        if USE_LABEL_SCALER:\n",
    "            label_scaler.fit(Y_train)  # Not used for this Classification task\n",
    "\n",
    "        # Save the fitted scaler object\n",
    "        try:\n",
    "            joblib.dump(feature_scaler, FEATURE_SCALER_PATH)\n",
    "            logger.info(f\"Feature scaler stored in: ({FEATURE_SCALER_PATH})\")\n",
    "            # Save the Label Scaler if utilized\n",
    "            if USE_LABEL_SCALER:\n",
    "                joblib.dump(\n",
    "                    label_scaler, LABEL_SCALER_PATH\n",
    "                )  # Not used for this Classification task\n",
    "                logger.info(f\"Label scaler stored in: ({LABEL_SCALER_PATH})\")\n",
    "        except FileNotFoundError as e:\n",
    "            raise RuntimeError(f\"Save path not found: {e}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An unexpected error occurred when saving  Scaler(s): {e}\")\n",
    "\n",
    "        # Scale all Feature Inputs\n",
    "        X_train_scaled = feature_scaler.transform(X_train)\n",
    "        X_validation_scaled = feature_scaler.transform(X_validation)\n",
    "        X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "        if USE_LABEL_SCALER:  # HANDLE EACH ON A CASE BY CASE BASIS\n",
    "            Y_train = label_scaler.transform(Y_train)\n",
    "            Y_validation = label_scaler.transform(Y_validation)\n",
    "            Y_test = label_scaler.transform(Y_test)\n",
    "\n",
    "        logger.info(f\"Train Features (Scaled) Shape: {X_train_scaled.shape}\")\n",
    "        logger.info(f\"Validation Features (Scaled) Shape: {X_validation_scaled.shape}\")\n",
    "        logger.info(f\"Test Features (Scaled) Shape: {X_test_scaled.shape}\")\n",
    "\n",
    "        if USE_LABEL_SCALER:\n",
    "            logger.info(f\"Train Labels (Scaled) Shape: {Y_train.shape}\")\n",
    "            logger.info(f\"Validation Labels (Scaled) Shape: {Y_validation.shape}\")\n",
    "            logger.info(f\"Test Labels (Scaled) Shape: {Y_test.shape}\")\n",
    "        else:\n",
    "            logger.info(f\"Train Labels Shape: {Y_train.shape}\")\n",
    "            logger.info(f\"Validation Labels Shape: {Y_validation.shape}\")\n",
    "            logger.info(f\"Test Labels Shape: {Y_test.shape}\")\n",
    "\n",
    "        # Define the column names of the features and label\n",
    "        features_names = df_features.columns\n",
    "        label_name = df_labels.columns\n",
    "\n",
    "        # Create dataframes using the scaled data\n",
    "        X_train_df = pd.DataFrame(X_train_scaled, columns=features_names)\n",
    "        X_test_df = pd.DataFrame(X_test_scaled, columns=features_names)\n",
    "        X_validation_df = pd.DataFrame(X_validation_scaled, columns=features_names)\n",
    "        Y_train_df = pd.DataFrame(Y_train, columns=label_name)\n",
    "        Y_test_df = pd.DataFrame(Y_test, columns=label_name)\n",
    "        Y_validation_df = pd.DataFrame(Y_validation, columns=label_name)\n",
    "\n",
    "        # Concatenate the features and labels back into a single DataFrame for each set\n",
    "        train_data_frame = pd.concat([X_train_df, Y_train_df.reset_index(drop=True)], axis=1)\n",
    "        test_data_frame = pd.concat([X_test_df, Y_test_df.reset_index(drop=True)], axis=1)\n",
    "        validation_data_frame = pd.concat(\n",
    "            [X_validation_df, Y_validation_df.reset_index(drop=True)], axis=1\n",
    "        )\n",
    "\n",
    "        # Saving the split data to csv files\n",
    "        try:\n",
    "            train_data_frame.to_csv(TRAIN_DATA_PATH, index=False)\n",
    "            test_data_frame.to_csv(TEST_DATA_PATH, index=False)\n",
    "            validation_data_frame.to_csv(VALIDATION_DATA_PATH, index=False)\n",
    "        except FileNotFoundError as e:\n",
    "            raise RuntimeError(f\"Save path not found: {e}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"An unexpected error occurred when saving datasets to CSV files:\\n{e}\"\n",
    "            )\n",
    "\n",
    "    # Creating Datasets from the stored datasets\n",
    "    logger.info(f\"INITIALIZING DATASETS\")\n",
    "    train_dataset = CustomDataset(csv_file=TRAIN_DATA_PATH, label_column=target_column)\n",
    "    test_dataset = CustomDataset(csv_file=TEST_DATA_PATH, label_column=target_column)\n",
    "    val_dataset = CustomDataset(csv_file=VALIDATION_DATA_PATH, label_column=target_column)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Creating DataLoaders with 'batch_size'=({batch_size}), 'num_workers'=({num_workers}), 'pin_memory'=({pin_memory}). Training dataset 'drop_last'=({drop_last})\"\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    validation_dataloader = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        f\"Training DataLoader has ({len(train_dataloader)}) batches, Test DataLoader has ({len(test_dataloader)}) batches, Validation DataLoader has ({len(validation_dataloader)}) batches\"\n",
    "    )\n",
    "\n",
    "    logger.info(\"==================================================================\")\n",
    "    for name, dataloader in [\n",
    "        (\"Train\", train_dataloader),\n",
    "        (\"Validation\", validation_dataloader),\n",
    "        (\"Test\", test_dataloader),\n",
    "    ]:\n",
    "        features, labels = next(iter(dataloader))  # Get one batch\n",
    "\n",
    "        logger.info(f\"{name} Dataloader Batch Information\")\n",
    "        logger.info(f\"Features Shape: '{features.shape}' |  DataTypes: '{features.dtype}'\")\n",
    "        logger.info(f\"Labels Shape: '{labels.shape}'   |  DataTypes: '{labels.dtype}' \")\n",
    "        logger.info(\"==================================================================\")\n",
    "\n",
    "    return (\n",
    "        train_dataset,\n",
    "        test_dataset,\n",
    "        val_dataset,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        validation_dataloader,\n",
    "        feature_scaler,\n",
    "        label_scaler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d6b97a",
   "metadata": {},
   "source": [
    "# Testing the Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c166aa6",
   "metadata": {},
   "source": [
    "## Testing with a given URL\n",
    "\n",
    "- Edit the Python dictionary 'data' section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a6147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USED WHEN TESTING THE RAW DATASET\n",
    "def test_data_pipeline():\n",
    "    # Function input setup\n",
    "    data = {\n",
    "        \"dataset_url\": \"REPLACE WITH THE URL OF THE DATASET\",\n",
    "        \"root_data_dir\": \"../Data\",\n",
    "        \"data_file_path\": DATA_CLEAN_FILE_NAME,\n",
    "        \"data_splits_dir\": \"DataSplits\",\n",
    "        \"scaler_dir\": \"Scalers\",\n",
    "        \"target_column\": \"REPLACE WITH TARGET/LABEL COLUMN NAME (ex. 'Price')\",\n",
    "        \"extra_dropped_columns\": [\n",
    "            # REPLACE WITH ANY COLUMNS TO BE EXCLUDED FROM THE DATASET - COMMA SEPARATED\n",
    "            \"COLUMN_1\",\n",
    "            \"COLUMN_2\",\n",
    "            \"ETC.\",\n",
    "        ],\n",
    "    }\n",
    "    batch_size = 64\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "    drop_last = True\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Call the data pipeline function\n",
    "    try:\n",
    "        (\n",
    "            train_dataset,\n",
    "            test_dataset,\n",
    "            val_dataset,\n",
    "            train_dataloader,\n",
    "            test_dataloader,\n",
    "            validation_dataloader,\n",
    "            feature_scaler,\n",
    "            label_scaler,\n",
    "        ) = data_pipeline(\n",
    "            logger,\n",
    "            **data,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=drop_last,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Caught Exception: {e}\", stack_info=True)\n",
    "\n",
    "    # Basic assertions to verify the outputs\n",
    "    assert isinstance(train_dataset, Dataset), \"train_dataset is not an instance of Dataset\"\n",
    "    assert isinstance(test_dataset, Dataset), \"test_dataset is not an instance of Dataset\"\n",
    "    assert isinstance(val_dataset, Dataset), \"val_dataset is not an instance of Dataset\"\n",
    "    assert isinstance(\n",
    "        train_dataloader, DataLoader\n",
    "    ), \"train_dataloader is not an instance of DataLoader\"\n",
    "    assert isinstance(\n",
    "        test_dataloader, DataLoader\n",
    "    ), \"test_dataloader is not an instance of DataLoader\"\n",
    "    assert isinstance(\n",
    "        validation_dataloader, DataLoader\n",
    "    ), \"validation_dataloader is not an instance of DataLoader\"\n",
    "    assert isinstance(\n",
    "        feature_scaler, MinMaxScaler\n",
    "    ), \"feature_scaler is not an instance of MinMaxScaler\"\n",
    "    # assert isinstance(label_scaler, MinMaxScaler), \"label_scaler is not an instance of MinMaxScaler\"\n",
    "\n",
    "    logger.info(\"All assertions passed. Data pipeline test successful.\")\n",
    "\n",
    "    return (\n",
    "        train_dataset,\n",
    "        test_dataset,\n",
    "        val_dataset,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        validation_dataloader,\n",
    "        feature_scaler,\n",
    "        label_scaler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d571ecb",
   "metadata": {},
   "source": [
    "### Call the 'test_data_pipeline' function and capture the return variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a32677",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    val_dataset,\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    validation_dataloader,\n",
    "    feature_scaler,\n",
    "    label_scaler,\n",
    ") = test_data_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72575b66",
   "metadata": {},
   "source": [
    "### Verify the length of the dataloader(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c5eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(validation_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafd10c7",
   "metadata": {},
   "source": [
    "### See details about a batch of each dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"==================================================================\")\n",
    "for name, dataloader in [\n",
    "    (\"Train\", train_dataloader),\n",
    "    (\"Validation\", validation_dataloader),\n",
    "    (\"Test\", test_dataloader),\n",
    "]:\n",
    "    features, labels = next(iter(dataloader))  # Get one batch\n",
    "\n",
    "    logger.info(f\"{name} Dataloader Batch Information\")\n",
    "    logger.info(f\"Features Shape: '{features.shape}' |  DataTypes: '{features.dtype}'\")\n",
    "    logger.info(f\"Labels Shape: '{labels.shape}'   |  DataTypes: '{labels.dtype}' \")\n",
    "    logger.info(f\"The labels: {labels}\")  # Optional\n",
    "    logger.info(\"==================================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
